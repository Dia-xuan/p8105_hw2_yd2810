---
title: "p8105_hw2_yd2810"
author: "Yaxuan Deng"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(dplyr)
library(tidyr)
library(lubridate)
library(readxl)
library(janitor)
```


## Problem 1

### 1.1

First, clean the data in pols-month.csv. Use separate() to break up the variable mon into integer variables year, month, and day; replace month number with month name; create a president variable taking values gop and dem, and remove prez_dem and prez_gop; and remove the day variable.

```{r}
path_pols <- file.path("data","pols-month.csv")

pols <- read_csv(path_pols) %>% 
  separate(mon, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day)
  ) %>% 
  mutate(month = month.name[month],
         president = case_when(prez_gop != 0 ~ "gop",
                               prez_dem != 0 ~ "dem")) %>% 
  select(year,month,president,everything(),-prez_gop,-prez_dem,-day)
```
### 1.2

Second, clean the data in snp.csv using a similar process to the above. For consistency across datasets, arrange according to year and month, and organize so that year and month are the leading columns.

```{r}
path_snp <- file.path("data", "snp.csv")

snp <- read_csv(path_snp) %>%
  mutate(
    date  = mdy(date),
    year  = year(date),
    month = month.name[month(date)] 
  ) %>%
  select(year, month, close) %>%
  arrange(year, match(month, month.name))
```
### 1.3

Third, tidy the unemployment data so that it can be merged with the previous datasets. This process will involve switching from “wide” to “long” format; ensuring that key variables have the same name; and ensuring that key variables take the same values.

```{r}
path_unem <- file.path("data", "unemployment.csv")

unemp <- read_csv(path_unem) %>% 
  pivot_longer(
    cols = -Year,
    names_to  = "month",  
    values_to = "unemployment"
  ) %>%
  mutate(
    month = month.name[match(month, month.abb)],
    year  = Year
  ) %>%
  select(year, month, unemployment) %>%
  arrange(year, match(month, month.name))
```
### 1.4 merging

Join the datasets by merging snp into pols, and merging unemployment into the result.

```{r}
pols_snp <- left_join(pols,snp,by = c("year","month"))
final <- left_join(pols_snp,unemp,by = c("year","month"))
```

Write a short paragraph about these datasets. Explain briefly what each dataset contained, and describe the resulting dataset (e.g. give the dimension, range of years, and names of key variables):

```{r}
n_obs <- nrow(final)
n_vars <- ncol(final)
y_min <- min(final$year)
y_max <- max(final$year)
```

`pols` contains the information of the number of national politicians who are democratic or republican at any given time. `snp` provides `r nrow(snp)` monthly S&P closing values. `unemp` records U.S. monthly unemployment percentages by year.

The resulting dataset has `r nrow(final)` rows (observations) and `r ncol(final)` variables, covering `r min(final$year)` to `r max(final$year)`. The key variables are `r paste(c("year","month","president","close","unemployment"), collapse = ", ")`.

## Problem 2

Read and clean the Mr. Trash Wheel sheet:
specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel
use reasonable variable names
omit rows that do not include dumpster-specific data
round the number of sports balls to the nearest integer and converts the result to an integer variable (using as.integer)

```{r}
path_tw <- file.path("data","202509 Trash Wheel Collection Data.xlsx")
mr <- read_excel(
  path_tw,
  sheet = "Mr. Trash Wheel",
  skip = 1
) %>% 
  select(where(~ !all(is.na(.)))) %>% 
  slice(-n()) %>% 
  clean_names() %>% 
  mutate(sports_balls = as.integer(round(sports_balls, 0)))

```

Use a similar process to import, clean, and organize the data for Professor Trash Wheel and Gwynnda, and combine this with the Mr. Trash Wheel dataset to produce a single tidy dataset. To keep track of which Trash Wheel is which, you may need to add an additional variable to both datasets before combining.

```{r}
professor <- read_excel(
  path_tw,
  sheet = "Professor Trash Wheel",
  skip = 1
) %>% 
  select(where(~ !all(is.na(.)))) %>% 
  slice(-n()) %>% 
  clean_names()

Gwynnda <- read_excel(
  path_tw,
  sheet = "Gwynns Falls Trash Wheel",
  skip = 1
) %>% 
  select(where(~ !all(is.na(.)))) %>% 
  slice(-n()) %>% 
  clean_names()
```


```{r}
mr <- mr %>%
  mutate(
    date = as.Date(date),
    year = as.integer(year),
    month = as.character(month),
    wheel = "Mr. Trash Wheel"
  ) %>%
  filter(!is.na(dumpster))

professor <- professor %>%
  mutate(
    date = as.Date(date),
    year = as.integer(year),
    month = as.character(month),
    wheel = "Professor Trash Wheel"
  ) %>%
  filter(!is.na(dumpster))

Gwynnda <- Gwynnda %>%
  mutate(
    date = as.Date(date),
    year = as.integer(year),
    month = as.character(month),
    wheel = "Gwynnda"
  ) %>%
  filter(!is.na(dumpster))

trash_all <- dplyr::bind_rows(mr, professor, Gwynnda) %>%
  select(wheel, dumpster, date, year, month, weight_tons, volume_cubic_yards,
         plastic_bottles, polystyrene, cigarette_butts, glass_bottles,
         sports_balls, dplyr::everything())

```


Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in the resulting dataset, and give examples of key variables. For available data, what was the total weight of trash collected by Professor Trash Wheel? What was the total number of cigarette butts collected by Gwynnda in June of 2022?

```{r}
prof_total_weight_tons <-
  trash_all %>% 
  filter(wheel == "Professor Trash Wheel") %>% 
  summarise(total = sum(weight_tons, na.rm = TRUE)) %>% 
  pull(total)

gwynnda_cigs_jun2022 <-
  trash_all %>% 
  filter(wheel == "Gwynnda",
         year(date) == 2022,
         month(date) == 6) %>% 
  summarise(total = sum(cigarette_butts, na.rm = TRUE)) %>% 
  pull(total)

```


The resulting dataset contains `r nrow(trash_all)` observations. Key variables include `wheel`,`dumpster` and `date`...Including information of pickup dates, dumpster IDs, total weight (tons), volume (cubic yards), and item counts such as cigarette butts and sports balls.

For the available data, Professor Trash Wheel collected a total of `r prof_total_weight_tons` tons of trash, and Gwynnda collected `r gwynnda_cigs_jun2022` cigarette butts in June 2022.

## Problem 3

```{r}
path_zori    <- file.path("data", "Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv")            
path_zipmeta <- file.path("data", "Zip Codes.csv") 
```

Create a single, well-organized dataset with all the information contained in these data files. To that end: import, clean, tidy, and otherwise wrangle each of these datasets; check for completeness and correctness across datasets (e.g. by viewing individual datasets and monitoring warning messages); merge to create a single, final dataset; and organize this so that variables and observations are in meaningful orders.

```{r}
zori_raw <- read_csv(path_zori) 
zip_raw <- read_csv(path_zipmeta)%>% clean_names() %>% 
  rename(zip = zip_code) %>% 
  mutate(zip = as.character(zip)) %>% 
  select(everything(),-file_date)
```
I removed constant (single-valued) variables — `RegionType`, `StateName`, `State`, `City`, and `Metro`.

```{r}
long_demo <- zori_raw %>% 
  pivot_longer(
    cols = 10:last_col(),
    names_to = "date",
    values_to = "zori",
    values_drop_na = TRUE
  ) %>% 
  rename(zip = RegionName) %>% 
  mutate(date = as.Date(date),
         zip = as.character(zip)) %>% 
  select(everything(),-RegionType,-StateName,-State,-City,-Metro)
```

```{r}

zips_zori  <- long_demo %>%  distinct(zip)
zips_meta  <- zip_raw   %>%  distinct(zip)

only_in_meta <- anti_join(zips_meta, zips_zori, by = "zip")

only_in_zori <- anti_join(zips_zori, zips_meta, by = "zip")

nrow(zips_zori); nrow(zips_meta)
nrow(only_in_meta); nrow(only_in_zori)
```
```{r}
zip_dups <- zip_raw %>%
  count(zip, name = "n") %>%
  filter(n > 1) %>%
  arrange(zip)

zip_dups
```

```{r}
dup_rows <- zip_raw %>%
  semi_join(zip_dups, by = "zip") %>%
  arrange(zip)

dup_rows
```
Two groups of same-zip-code. Both seem unreasonable. Delete two false observations.

```{r}
false_pairs <- tribble(
  ~zip,~county,
  "10463", "New York",
  "11201", "New York"
)

zip <- zip_raw %>% 
  anti_join(false_pairs, by = c("zip", "county")) %>% 
  mutate(
    borough = case_when(
      county == "Kings"    ~ "Brooklyn",
      county == "Queens"   ~ "Queens",
      county == "New York" ~ "Manhattan",
      county == "Bronx"    ~ "Bronx",
      county == "Richmond" ~ "Staten Island"))%>%
  select(zip, borough, county, neighborhood)
  
```

```{r}
zori_zip <- long_demo %>% 
  inner_join(zip,by = "zip") %>% 
  relocate(zip, borough, neighborhood, county, date, zori) %>%
  arrange(zip, date)
```

Answer:
The final tidy dataset zori_zip contains `r nrow(zori_zip)` observations and `r ncol(zori_zip)`. It includes `r n_distinct(zori_zip$zip)` unique ZIP codes and `r n_distinct(zori_zip$neighborhood)` unique neighborhoods across the five NYC boroughs. Key variables are zip, borough, neighborhood, date, and zori (the Zillow Observed Rent Index).

```{r}
zips_only_in_meta <- zip %>%
  distinct(zip) %>%
  anti_join(long_demo %>% distinct(zip), by = "zip")

zips_only_in_meta$zip           

examples_meta_only <- zip %>%
  semi_join(zips_only_in_meta, by = "zip") %>%
  distinct(zip, borough, county, neighborhood) %>%
  arrange(zip)

head(examples_meta_only, 10)
```
There are `r nrow(zips_only_in_meta)` zip codes that only appear in the Zip code dataset. 
Reasons may include: 1. Some ZIP codes may cover government facilities, parks, or campuses etc. with no market rental activity.
2. A ZIP that overlaps multiple neighborhoods or has ambiguous residential coverage can be excluded.


Rental prices fluctuated dramatically during the COVID-19 pandemic. For all available ZIP codes, compare rental prices in January 2021 to prices in January 2020. Make a table that shows the 10 ZIP codes (along with the borough and neighborhood) with largest drop in price from January 2020 to 2021. Comment.

```{r}
jan_drop <- zori_zip %>%
  filter(month(date) == 1, year(date) %in% c(2020, 2021)) %>%
  mutate(yr = year(date)) %>%
  select(zip, borough, neighborhood, yr, zori) %>%
  distinct() %>%
  pivot_wider(names_from = yr, values_from = zori, names_prefix = "zori_") %>%
  filter(!is.na(zori_2020), !is.na(zori_2021)) %>%
  mutate(change_2021_vs_2020 = zori_2021 - zori_2020) %>%
  arrange(change_2021_vs_2020) %>%    
  slice_head(n = 10)
```

The 10 largest drops are all in Manhattan, concentrated in Lower Manhattan, Lower East Side, Gramercy/Murray Hill, and Greenwich Village/SoHo.
